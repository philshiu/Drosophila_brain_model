# -*- coding: utf-8 -*-
"""2023_02_20 Sugar example 12 587 dros model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10P7b1qc3W6Hi5DJQEdIJOL-uIRp2-ujC
"""

#The goal of this file is to have a file that can be imported and run elsewhere.

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
import pickle

from brian2 import NeuronGroup, Synapses, PoissonInput, StateMonitor, SpikeMonitor, Network
from brian2 import mV, ms, Hz, Mohm, uF
import pickle

from pathlib import Path

# files to read/write
neurotransmitter_file_path    = './2023_02_01_completeness_587_final.csv'
connectivity_file_path = './2023_02_01_connectivity_587_final.parquet'

connectivity_dataframe = pd.parquet(connectivity_file_path, index_col = 0) #Connectivity information
neurotransmitter_dataframe = pd.read_csv(neurotransmitter_file_path, index_col = 0) # neuron ids and excitation type

pickle_map  = './name_mappings_587.pickle'
# load name mappings
with open(pickle_map, 'rb') as f:
    flyid2i, flyid2name, i2flyid, i2name, name2flyid, name2i = pickle.load(f)

# Our parameters of our model, from Skutt-Kakaria and de Bivort.

v_0     = -52 * mV          # resting potential
v_rst   = -52 * mV          # reset potential after spike
v_th    = -45 * mV          # threshold for spiking
r_mbr   = 10. * Mohm        # membrane resistance
t_rfc   = 2.2 * ms          # refractory period
c_mbr   = .002 * uF         # membrane capacitance 
t_mbr   = c_mbr * r_mbr     # membrane time scale
tau     = 5 * ms            # Kakaria TODO: insert citation
w_syn   = .1 * 2.75 * mV    # weight per synapse (note: modulated by exponential decay)
syn_delay = 1.8* ms          #Delay, the time from action potential to change in membrane potential, comes from Paul et al., 2015
# define basic equations
eqs = '''
dv/dt = (x - (v - v_0)) / t_mbr : volt (unless refractory)
dx/dt = -x / tau                : volt (unless refractory) 
'''
eq_th   = 'v > v_th' # condition for spike
eq_rst  = 'v = v_rst; w = 0; x = 0 * mV' # rules when spike 

def default_model(connectivity = connectivity_dataframe, neurotransmitter_data = neurotransmitter_dataframe ):
    '''create default model for neurons and synapses from flywire data
    relies on equations and parameters defined above
    returns NeuronGroup, Synapses
    '''
    
    neu = NeuronGroup( # create neurons
        N=len(neurotransmitter_data),
        model=eqs,
        method='linear',
        threshold=eq_th,
        reset=eq_rst,
        refractory=t_rfc,
        name='default_neurons', 
    )
    neu.v = v_0 # initialize values
    neu.x = 0

    # create synapses
    syn = Synapses(neu, neu, 'w : volt', on_pre='x += w', delay=syn_delay, name='default_synapses') 

    # connect synapses
    i_pre = connectivity.loc[:, 'Presynaptic_Index'].values
    i_post = connectivity.loc[:, 'Postsynaptic_Index'].values
    syn.connect(i=i_pre, j=i_post)

    # define connection weight
    syn.w = connectivity.loc[:,"Excitatory x Connectivity"].values * w_syn

    # object to record spikes
    spk_mon = SpikeMonitor(neu) 

    return neu, syn, spk_mon

# experimental setup
t_sim = 1000 * ms   # duration of trial
n_run = 30          # number of runs

# helper to save spikes
def save_spk(i_run, spk_mon, df):
    '''write spike times to dataframe
    creates a new column for each run'''

    spk_trn = {k: v for k, v in spk_mon.spike_trains().items() if len(v)} # select only non-empty spike trains
    df.loc[:, 'run_{}'.format(i_run) ] = pd.Series(spk_trn) # add to dataframe

    return df.copy() # return copy do avoid dataframe fragmentation

def create_poisson_input(neu, names, rate=150*Hz, delta = 10):
    'creates a list of PoissonInput objects for a list of neuron names and NeuronGroup neu'
    l = []
    for n in names:
        if isinstance(n, int): #If this is a Flywire ID, convert to index value
          i = flyid2i[n]
        if isinstance(n, str): #Convert the string into the index value
          i = name2i[n]
        p = PoissonInput(target=neu[i], target_var='v', N=1, rate=rate, weight=w_syn*delta)
        l.append(p)      
    return l

def run_activation_experiment(experiment_name, neurons_to_excite, activation_rate=500*Hz, delta = 10, connectivity = connectivity_dataframe, neurotransmitter_data = neurotransmitter_dataframe, 
                              output_path = './Activation_experiment_1/', ):
    '''
    supply name (experiment_name) and list of neuron names to excite (exc)'''

    output_file_path = output_path + 'experiment_{}.feather'.format(experiment_name)
    output_pickle_path = output_path + 'experiment_{}.pickle'.format(experiment_name)    

    print('>>> Experiment:     {}'.format(experiment_name))
    print('    Output files:   {}'.format(output_file_path))
    print('                    {}'.format(output_pickle_path))

    neu, syn, spk_mon = default_model(connectivity, neurotransmitter_data ) # get default network
    poi_inp = create_poisson_input(neu, neurons_to_excite, activation_rate, delta = delta) # define Poisson input for excitation
    net = Network(neu, syn, spk_mon, *poi_inp)  # define network
    net.store() # store initial state

    df_spk = pd.DataFrame(index=i2flyid.keys()) # empty dataframe to collect spike times
    for i in range(n_run):
        # run
        net.restore() # restore initial state
        
        net.run(duration=t_sim) # run simulation

        # save spike times
        df_spk = save_spk(i, spk_mon, df_spk)

    # store spike times
    df_spk.to_feather(output_file_path)

    # store metadata
    meta_data = {
        'experiment_name':      experiment_name,
        'neurons_to_excite':      neurons_to_excite,
        't_sim':    t_sim,
        'n_run':    n_run,
    }
    with open(output_pickle_path, 'wb') as f:
        pickle.dump(meta_data, f)

def load_exp(pkl_glob):
    'load experiments from pickle and feather'

    meta_dict, spk_dict = dict(), dict()

    for pkl in pkl_glob:
        # exp name
        exp = pkl.name.replace('.pickle', '').replace('experiment_', '')

        # load metadata from pickle
        with open(pkl, 'rb') as f:
            data = pickle.load(f) 
        meta_dict[exp] = data

        # load spike times from feather
        fth = pkl.with_suffix('.feather')
        spk_dict[exp] = pd.read_feather(fth)

    return meta_dict, spk_dict

def analyze_activation_experiment(input_folder, t_sim = 1, n_run = 30 ):
  '''
  Takes a path to a directory, input_folder with experiment data; t_sim, the time, in seconds of each simulations, and the number of simulations, and returns the mean spiking time of all neurons that fired, as well as standard deviation of spiking
  '''
  # wildcard matching multiple outputs
  pkl_glob = Path(input_folder).glob('experiment_*.pickle') 

  # get two dictionaries: meta data and spike dataframes
  meta_dict, spk_dict = load_exp(pkl_glob) 

  # create another dict with list of excited neurons per experiment. Useful if you want to exclude these neurons.
  #exc_dict = { i: j['exc'] for i, j in meta_dict.items() }

  # experimental setup TODO: do not hard code
  t_sim = 1           # duration of trial in s 
  n_run = 30          # number of runs


  # print info
  print('Loaded {} experiments...'.format(len(meta_dict)))
  print('... ' + ' '.join([i for i in meta_dict.keys()]))
  print()
  print('Loaded {} name mapping... (showing first 10)'.format(len(name2flyid)))
  print('... ' + ' '.join([i for i in name2flyid.keys()][:10]) + ' ...')

  # collect spike times of all experiments in one dataframe
  df = pd.DataFrame() #For collecting data on means
  df_std = pd.DataFrame() #For collecting data on standard deviations

  for exp in spk_dict: # cycle through experiments
      df_exp = spk_dict[exp] # df for individual experiment
      
      #  sum up spike events
      df_exp = df_exp.dropna(how='all') # select only rows containing spikes
      ds_mean = df_exp.apply(lambda x: np.concatenate(x.dropna(how='any')), axis=1) # concatenate spk times, ignore nan
      ds_stdev = df_exp.applymap( lambda x: len(x), na_action='ignore' ) # Calculate length of each array, that is, number of times each neuron spiked.
      ds_stdev = ds_stdev.std(axis =1) #Calculates the Std deviation.

      ds_mean.name = exp # dataseries name will convert to dataframe column
      ds_stdev.name = exp

      df = pd.concat([df, ds_mean], axis=1)
      df_std = pd.concat([df_std , ds_stdev], axis=1)

  # rename brian ids to flywire/custom names
  df = df.rename(index=i2flyid).rename(index=flyid2name) # (1) flywire ids and (2) custom names
  df.index = df.index.astype(str) # represent flywire IDs as str, not int

  # convert counts to rates
  df_rate = df.applymap( lambda x: len(x) / ( n_run * t_sim ), na_action='ignore' )
  df_rate = df_rate.fillna(0) # replace nan with 0, necessary for differences later

  df_std  = df_std.rename(index=i2flyid).rename(index=flyid2name) # (1) flywire ids and (2) custom names
  df_std.index = df_std.index.astype(str) # represent flywire IDs as str, not int

  df_std  = df_std.fillna(0) # replace nan with 0, necessary for differences later

  df_std = df_std.add_suffix('_stdev')

  df_all = pd.concat([df_rate, df_std], axis = "columns")
  return(df_all)

def silencing_model(Neuron_to_silence,  connectivity_path = './2023_02_01_connectivity_587_final.csv'):
    '''create default model for neurons and synapses from flywire data
    relies on equations and parameters defined above
    returns NeuronGroup, Synapses
    '''
    df_con = pd.parquet(connectivity_file_path, index_col = 0)
    if Neuron_to_silence > 3:
      df_con.loc[df_con["Presynaptic_ID"] == Neuron_to_silence, "Excitatory x Connectivity"]=0

    neu = NeuronGroup( # create neurons
        N=len(neurotransmitter_dataframe),
        model=eqs,
        method='linear',
        threshold=eq_th,
        reset=eq_rst,
        refractory=t_rfc,
        name='default_neurons', 
    )
    neu.v = v_0 # initialize values
    neu.x = 0

    # create synapses
    syn = Synapses(neu, neu, 'w : volt', on_pre='x += w', delay=1.8*ms, name='default_synapses')

    # connect synapses
    i_pre = df_con.loc[:, 'Presynaptic_Index'].values
    i_post = df_con.loc[:, 'Postsynaptic_Index'].values
    syn.connect(i=i_pre, j=i_post)

    # define connection weight
    syn.w = df_con.loc[:,"Excitatory x Connectivity"].values * w_syn

    # object to record spikes
    spk_mon = SpikeMonitor(neu) 

    return neu, syn, spk_mon

def run_silencing_experiment(experiment_name, neurons_to_excite, neurons_to_silence, activation_rate=500*Hz, delta = 10, connectivity_path = './2023_02_01_connectivity_587_final.csv', 
                              t_sim = t_sim, output_path = './Silencing_test_1/', ):
    '''
    supply name (experiment_name) and list of neuron names to excite (exc)'''

    output_file_path = output_path + 'experiment_{}.feather'.format(experiment_name)
    output_pickle_path = output_path + 'experiment_{}.pickle'.format(experiment_name)    

    print('>>> Experiment:     {}'.format(experiment_name))
    print('    Output files:   {}'.format(output_file_path))
    print('                    {}'.format(output_pickle_path))

    neu, syn, spk_mon = silencing_model(neurons_to_silence, connectivity_path ) # get default network
    poi_inp = create_poisson_input(neu, neurons_to_excite, activation_rate, delta = delta) # define Poisson input for excitation
    net = Network(neu, syn, spk_mon, *poi_inp)  # define network
    net.store() # store initial state

    df_spk = pd.DataFrame(index=i2flyid.keys()) # empty dataframe to collect spike times
    for i in range(n_run):
        # run
        net.restore() # restore initial state
        
        net.run(duration=t_sim) # run simulation

        # save spike times
        df_spk = save_spk(i, spk_mon, df_spk)

    # store spike times
    df_spk.to_feather(output_file_path)

    # store metadata
    meta_data = {
        'experiment_name':      experiment_name,
        'neurons_to_excite':      neurons_to_excite,
        't_sim':    t_sim,
        'n_run':    n_run,
    }
    with open(output_pickle_path, 'wb') as f:
        pickle.dump(meta_data, f)